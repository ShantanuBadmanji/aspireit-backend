{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0e8a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: In the field of artificial intelligence, the concepts of weak AI and strong AI represent distinctly different levels of machine intelligence. Can you elaborate on these differences, providing specific examples of where each might be applied, and discuss potential challenges that might arise when transitioning from the development of weak AI systems to strong AI systems?\n",
      "Recording for 15 seconds. Please answer the question...\n"
     ]
    },
    {
     "ename": "PortAudioError",
     "evalue": "Error querying device -1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 191\u001b[0m\n\u001b[1;32m    186\u001b[0m topics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the difference between weak AI and strong AI?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    188\u001b[0m ]\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Run the interview process\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m final_report \u001b[38;5;241m=\u001b[39m \u001b[43mconduct_interview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecording_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_report)\n",
      "Cell \u001b[0;32mIn[1], line 110\u001b[0m, in \u001b[0;36mconduct_interview\u001b[0;34m(topics, recording_duration)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Record response\u001b[39;00m\n\u001b[1;32m    109\u001b[0m audio_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecorded_response_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 110\u001b[0m \u001b[43mrecord_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecording_duration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     candidate_response \u001b[38;5;241m=\u001b[39m speech_to_text(audio_filename)\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mrecord_audio\u001b[0;34m(filename, duration, sample_rate)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecord_audio\u001b[39m(filename, duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds. Please answer the question...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     recording \u001b[38;5;241m=\u001b[39m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     sd\u001b[38;5;241m.\u001b[39mwait()  \u001b[38;5;66;03m# Wait until recording is finished\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     wav\u001b[38;5;241m.\u001b[39mwrite(filename, sample_rate, recording)\n",
      "File \u001b[0;32m/mnt/d/SHANTANU WORKSPACE/aspireit/aspireit backend/aspireit backend/env/lib/python3.10/site-packages/sounddevice.py:276\u001b[0m, in \u001b[0;36mrec\u001b[0;34m(frames, samplerate, channels, dtype, out, mapping, blocking, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mread_indata(indata)\n\u001b[1;32m    274\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mcallback_exit()\n\u001b[0;32m--> 276\u001b[0m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mInputStream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/mnt/d/SHANTANU WORKSPACE/aspireit/aspireit backend/aspireit backend/env/lib/python3.10/site-packages/sounddevice.py:2605\u001b[0m, in \u001b[0;36m_CallbackContext.start_stream\u001b[0;34m(self, StreamClass, samplerate, channels, dtype, callback, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m, StreamClass, samplerate, channels, dtype, callback,\n\u001b[1;32m   2603\u001b[0m                  blocking, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2604\u001b[0m     stop()  \u001b[38;5;66;03m# Stop previous playback/recording\u001b[39;00m\n\u001b[0;32m-> 2605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[43mStreamClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2608\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2609\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinished_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinished_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   2612\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _last_callback\n",
      "File \u001b[0;32m/mnt/d/SHANTANU WORKSPACE/aspireit/aspireit backend/aspireit backend/env/lib/python3.10/site-packages/sounddevice.py:1429\u001b[0m, in \u001b[0;36mInputStream.__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1400\u001b[0m              device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, latency\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1401\u001b[0m              extra_settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, finished_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1402\u001b[0m              clip_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, never_drop_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1403\u001b[0m              prime_output_buffers_using_stream_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"PortAudio input stream (using NumPy).\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \n\u001b[1;32m   1406\u001b[0m \u001b[38;5;124;03m    This has the same methods and attributes as `Stream`, except\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \n\u001b[1;32m   1428\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1429\u001b[0m     \u001b[43m_StreamBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_remove_self\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/SHANTANU WORKSPACE/aspireit/aspireit backend/aspireit backend/env/lib/python3.10/site-packages/sounddevice.py:825\u001b[0m, in \u001b[0;36m_StreamBase.__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    822\u001b[0m         samplerate \u001b[38;5;241m=\u001b[39m isamplerate\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m     parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize, samplerate \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 825\u001b[0m         \u001b[43m_get_stream_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mextra_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39mchannelCount\n",
      "File \u001b[0;32m/mnt/d/SHANTANU WORKSPACE/aspireit/aspireit backend/aspireit backend/env/lib/python3.10/site-packages/sounddevice.py:2683\u001b[0m, in \u001b[0;36m_get_stream_parameters\u001b[0;34m(kind, device, channels, dtype, latency, extra_settings, samplerate)\u001b[0m\n\u001b[1;32m   2680\u001b[0m     samplerate \u001b[38;5;241m=\u001b[39m default\u001b[38;5;241m.\u001b[39msamplerate\n\u001b[1;32m   2682\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_id(device, kind, raise_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2683\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mquery_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2685\u001b[0m     channels \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m kind \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_channels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/mnt/d/SHANTANU WORKSPACE/aspireit/aspireit backend/aspireit backend/env/lib/python3.10/site-packages/sounddevice.py:569\u001b[0m, in \u001b[0;36mquery_devices\u001b[0;34m(device, kind)\u001b[0m\n\u001b[1;32m    567\u001b[0m info \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mPa_GetDeviceInfo(device)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m info:\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PortAudioError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError querying device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m info\u001b[38;5;241m.\u001b[39mstructVersion \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    571\u001b[0m name_bytes \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mstring(info\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mPortAudioError\u001b[0m: Error querying device -1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set OpenAI API Key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert OPENAI_API_KEY, \"Please set your OpenAI API Key in a .env file\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Ensure the directory containing ffmpeg is in the PATH environment variable\n",
    "os.environ['PATH'] += os.pathsep + '/opt/homebrew/bin'\n",
    "\n",
    "# Load Whisper Model for Transcribing Audio\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "# Record audio in real-time\n",
    "def record_audio(filename, duration=15, sample_rate=16000):\n",
    "    print(f\"Recording for {duration} seconds. Please answer the question...\")\n",
    "    recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    wav.write(filename, sample_rate, recording)\n",
    "    print(f\"Recording completed and saved as {filename}.\")\n",
    "\n",
    "# Function to Transcribe Audio Files\n",
    "def speech_to_text(audio_file):\n",
    "    result = whisper_model.transcribe(audio_file)\n",
    "    return result['text']\n",
    "\n",
    "# Function to Generate GPT-4 Questions\n",
    "def generate_question(topic):\n",
    "    prompt = f\"Generate a complex interview question for an AI engineer about {topic}.\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "        temperature=0.6\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Function to Generate Reference Answers\n",
    "def generate_reference_answer(question):\n",
    "    prompt = f\"Provide a comprehensive answer for the following AI engineering interview question: {question}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.6\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Function to Compare and Score Candidate Answers\n",
    "def compare_answers(candidate_answer, reference_answer):\n",
    "    matcher = SequenceMatcher(None, candidate_answer, reference_answer)\n",
    "    similarity_score = matcher.ratio()\n",
    "    return similarity_score\n",
    "\n",
    "# Function to Categorize Answers\n",
    "def categorize_response(candidate_response, reference_answer):\n",
    "    important_keywords = extract_keywords(reference_answer)\n",
    "    response_keywords = extract_keywords(candidate_response)\n",
    "    keyword_match_ratio = len(set(response_keywords).intersection(important_keywords)) / len(important_keywords)\n",
    "    category = \"Accurate\" if keyword_match_ratio > 0.7 else \"General\" if keyword_match_ratio > 0.4 else \"Inaccurate\"\n",
    "    return category, keyword_match_ratio\n",
    "\n",
    "# Keyword Extraction (Basic Example)\n",
    "def extract_keywords(text):\n",
    "    stop_words = set([\"the\", \"is\", \"in\", \"and\", \"to\", \"with\", \"for\", \"on\", \"by\", \"an\", \"of\", \"a\", \"as\"])\n",
    "    keywords = [word for word in text.lower().split() if word not in stop_words]\n",
    "    return set(keywords)\n",
    "\n",
    "# Fine-tuned Evaluation Factors\n",
    "def evaluate_candidate_response(candidate_response, reference_answer):\n",
    "    relevance_score = compare_answers(candidate_response, reference_answer)\n",
    "    critical_thinking_score = relevance_score * 10  # Enhanced calculation\n",
    "    communication_score = min(len(candidate_response.split()) / len(reference_answer.split()), 1.0)  # Normalize to [0, 1]\n",
    "    depth_of_answer_score = min(len(candidate_response) / len(reference_answer), 1.0)  # Normalize to [0, 1]\n",
    "    coherence_score = 1 if candidate_response.count('.') >= reference_answer.count('.') else 0.5\n",
    "\n",
    "    return {\n",
    "        \"Relevance Score\": relevance_score,\n",
    "        \"Critical Thinking\": critical_thinking_score,\n",
    "        \"Communication Skills\": communication_score,\n",
    "        \"Depth of Answer\": depth_of_answer_score,\n",
    "        \"Coherence\": coherence_score\n",
    "    }\n",
    "\n",
    "# Conduct the Full Interview Process and Evaluation\n",
    "def conduct_interview(topics, recording_duration=90):\n",
    "    results = []\n",
    "    report_data = defaultdict(list)\n",
    "\n",
    "    for idx, topic in enumerate(topics):\n",
    "        # Generate the question and display it\n",
    "        question = generate_question(topic)\n",
    "        print(f\"Question {idx + 1}: {question}\")\n",
    "\n",
    "        # Record response\n",
    "        audio_filename = f'recorded_response_{idx + 1}.wav'\n",
    "        record_audio(audio_filename, duration=recording_duration)\n",
    "\n",
    "        try:\n",
    "            candidate_response = speech_to_text(audio_filename)\n",
    "            reference_answer = generate_reference_answer(question)\n",
    "\n",
    "            # Analyze Response\n",
    "            category, keyword_match_ratio = categorize_response(candidate_response, reference_answer)\n",
    "            scores = evaluate_candidate_response(candidate_response, reference_answer)\n",
    "            scores[\"Category\"] = category\n",
    "            scores[\"Keyword Match Ratio\"] = keyword_match_ratio\n",
    "            results.append({\n",
    "                \"Question\": question,\n",
    "                \"Transcribed Answer\": candidate_response,\n",
    "                \"Reference Answer\": reference_answer,\n",
    "                \"Scores\": scores\n",
    "            })\n",
    "\n",
    "            # Add scores to report data\n",
    "            for key, value in scores.items():\n",
    "                report_data[key].append(value)\n",
    "\n",
    "            # Generate visualization for each question\n",
    "            generate_question_visualization(question, scores, idx + 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {audio_filename}: {str(e)}\")\n",
    "\n",
    "    # Generate the Final Report\n",
    "    final_report = generate_report(report_data, results)\n",
    "    return final_report\n",
    "\n",
    "# Generate Visualization for Each Question\n",
    "def generate_question_visualization(question, scores, question_number):\n",
    "    df = pd.DataFrame([scores])\n",
    "    df.set_index(pd.Index([question_number]))\n",
    "\n",
    "    # Plot bar chart for scores\n",
    "    df.plot(kind='bar', figsize=(10, 6), legend=True)\n",
    "    plt.title(f\"Evaluation Scores for Question {question_number}\")\n",
    "    plt.xticks([])\n",
    "    plt.xlabel(question)\n",
    "    plt.ylabel('Scores')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(f\"evaluation_question_{question_number}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Generate Report and Visualization\n",
    "def generate_report(report_data, results):\n",
    "    df = pd.DataFrame(report_data)\n",
    "\n",
    "    # Filter out non-numeric columns for calculating overall score\n",
    "    numeric_cols = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Overall Interview Score\n",
    "    if not numeric_cols.empty:\n",
    "        overall_score = numeric_cols.mean().mean()\n",
    "    else:\n",
    "        overall_score = None\n",
    "\n",
    "    # Visual Representation (e.g., Box Plot)\n",
    "    numeric_cols.plot(kind='box', figsize=(10, 6))\n",
    "    plt.title(\"Overall Interview Evaluation Report\")\n",
    "    plt.savefig(\"overall_evaluation_report.png\")\n",
    "\n",
    "    # Structure the final report\n",
    "    final_report = {\n",
    "        \"Overall Interview Score\": overall_score,\n",
    "        \"Question-wise Scores\": numeric_cols.to_dict(orient='list'),\n",
    "        \"Overall Visual Representation\": \"overall_evaluation_report.png\",\n",
    "        \"Detailed Results\": results\n",
    "    }\n",
    "\n",
    "    return final_report\n",
    "\n",
    "# Example Usage\n",
    "topics = [\n",
    "    \"What is the difference between weak AI and strong AI?\",\n",
    "]\n",
    "\n",
    "# Run the interview process\n",
    "final_report = conduct_interview(topics, recording_duration=15)\n",
    "print(final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b501c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
